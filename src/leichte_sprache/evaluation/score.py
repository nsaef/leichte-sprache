import os
from statistics import mean

from datasets import load_dataset
import evaluate
from lingua import Language, LanguageDetectorBuilder
import spacy
import textstat
import torch
from transformers import PreTrainedModel, PreTrainedTokenizer


def calculate_readability_scores(text: str) -> dict:
    """Calculate readability scores on a German text.

    Flesch reading ease: the higher, the easier. Maximum ist 121.22.

    :param text: input text
    :return: dict with the metric names as keys and the scores as values
    """

    textstat.set_lang("de")
    flesch_reading_ease = textstat.flesch_reading_ease(text)
    wiener_sachtextformel = textstat.wiener_sachtextformel(text, variant=4)

    res = {
        "flesch_reading_ease": flesch_reading_ease,
        "wiener_sachtextformel_4": wiener_sachtextformel,
    }
    return res


def calculate_rouge(predictions: list[str], references: list[str]) -> list[float]:
    """Calculate rouge2 between two lists of texts.

    :param predictions: list of predicted texts
    :param references: list of reference texts
    :return: list of rouge2 scores
    """
    rouge = evaluate.load("rouge")
    scores = rouge.compute(
        predictions=predictions,
        references=references,
        rouge_types=["rouge2"],
        use_aggregator=False,
    )
    return scores["rouge2"]


def recognize_language(texts: list[str]) -> list[str]:
    """Run automated language recognition on a list of texts, such as  `GERMAN` or `ENGLISH`.

    :param texts: list of texts
    :return: list of language names
    """
    languages = [Language.ENGLISH, Language.GERMAN]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()
    langs = detector.detect_languages_in_parallel_of(texts)
    lang_strings = [l.name if l else None for l in langs]
    return lang_strings


def split_text_naive(
    text: str, sep: str = None, separate_hyphens: bool = True
) -> list[str]:
    """Naive word splitting. Splits the text at the given separator and returns a list of words.
    Defaults to splitting at whitespace. By default, also replaces hyphens (which are commonly
    inserted into long words in Leichte Sprache) with the separator to account for the added
    readibility generated by this spelling.

    :param text: input string
    :param sep: separator. Default: None
    :param separate_hyphens: replace hyphens with the separator token to count hyphenated words as multiple words. Default:True
    :return: list of words
    """
    if separate_hyphens:
        replacement = " " if sep is None else sep
        text = text.replace("-", replacement)
    split = text.split(sep)
    return split


def calc_mean_word_length(split_text: list[str]) -> float:
    """Calculate the mean word length of a split (tokenized) text.
    Split the text using `score.split_text_naive` with your preferred
    parameters before using this function.

    :param split_text: text split into words
    :return: mean word length across the whole text
    """
    result = mean([len(w) for w in split_text])
    return result


def get_spacy_doc(text: str) -> spacy.Language:
    """Create a spacy document from a text.

    :param text: Input text
    :return: spacy doc
    """
    nlp = spacy.load(
        "de_core_news_sm", exclude=["ner", "tagger", "parser," "lemmatizer"]
    )
    doc = nlp(text)
    return doc


def calc_mean_sents_per_paragraph(paragraphs: list[list[str]]) -> float:
    """Take a list of paragraphs, each containing a list of the sentences
    in the paragraph. Calculate the mean number of sentences inside the paragraphs.

    :param paragraphs: list of sentences in each paragraph. Example: [["this is the first paragraph", "it has two sentences"], ["this is paragraph nr. 2"]]
    :return: mean paragraph length in sentences
    """
    if not paragraphs:
        return 0
    n_sents = [len(p) for p in paragraphs]
    mean_sents = mean(n_sents)
    return mean_sents


def split_paragraphs(sents: list) -> list[list[str]]:
    """Split a list of spacy sentences into a list of sentences per paragraph.

    :param sents: list of doc.sents
    :return: a list containing, for each paragraph, a list of the sentences in the paragraph
    """
    paragraphs = []

    current_par = []
    for sent in sents:
        current_par.append(sent)
        if "\n" in sent.text:
            paragraphs.append(current_par)
            current_par = []
    return paragraphs


def analyse_text_statistics(text: str) -> dict:
    """Run some quick statistics on the current text. Currently implemented are:

    - total number of sentences in the text
    - mean word length
    - mean paragraph length (in sentences)

    :param text: input text
    :return: dict with the metric names as keys and the results as values
    """

    # preprocess text
    naive_split = split_text_naive(text)
    doc = get_spacy_doc(text)
    sents = [sent for sent in doc.sents]
    paragraphs = split_paragraphs(sents)

    # calculate statistics
    mean_word_length = calc_mean_word_length(naive_split)
    total_sents_normalized = len(sents) / len(text)
    mean_sents_per_paragraph = calc_mean_sents_per_paragraph(paragraphs=paragraphs)

    res = {
        "total_sent_number_normalized": total_sents_normalized,
        "mean_word_length": mean_word_length,
        "mean_sents_per_paragraph": mean_sents_per_paragraph,
    }
    return res


def score_classification_set():
    # get the dataset
    dataset = load_dataset(os.getenv("HF_CLASSIFICATION_DATASET_NAME"), split="train")

    # run readibility, lexical diversity, avg sentence length, avg word length, avg text length?
    # dataset = dataset.map(lambda x: analyse_text_statistics(x["text"])) - DISABLED, NOT HELPFUL ENOUGH
    dataset = dataset.map(lambda x: calculate_readability_scores(x["text"]))
    dataset = dataset.train_test_split(test_size=0.1)

    dataset["train"].push_to_hub(
        os.getenv("HF_CLASSIFICATION_DATASET_NAME"),
        token=os.getenv("HF_TOKEN"),
        split="train",
    )
    dataset["test"].push_to_hub(
        os.getenv("HF_CLASSIFICATION_DATASET_NAME"),
        token=os.getenv("HF_TOKEN"),
        split="validation",
    )
    return


def run_rule_based_checks():
    # use short words, separate all long words with a hyphen
    # no abbreviations
    # use verbs rather than nouns
    # use active rather than passive
    # avoid genitive forms
    # avoid conjunctive forms
    # avoid negations
    # use arabic numbers only
    # avoid precise dates which are far in the past
    # avoid precise numbers (floating point, percentages)
    # prefer numbers (1) over number-words (one)
    # avoid special characters like " % ... ; & () $
    pass


def run_classifier(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, text: str):
    # todo docs
    encoded_input = tokenizer(text, return_tensors="pt")
    labels = torch.tensor([1]).unsqueeze(0)
    output = model(**encoded_input, labels=labels)
    predicted_class_id = output.logits.argmax().item()
    return predicted_class_id, output.logits


if __name__ == "__main__":
    score_classification_set()

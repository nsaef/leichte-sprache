import os
import re
from statistics import mean

from datasets import load_dataset
import evaluate
from lingua import Language, LanguageDetectorBuilder
import pandas as pd
import spacy
import textstat
from tqdm import tqdm
import torch
from transformers import (
    PreTrainedModel,
    PreTrainedTokenizer,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)

from leichte_sprache.utils.model_utils import chunk_text
from leichte_sprache.utils.utils import get_logger


logger = get_logger()


def calculate_readability_scores(text: str) -> dict:
    """Calculate readability scores on a German text.

    Flesch reading ease: the higher, the easier. Maximum ist 121.22.

    :param text: input text
    :return: dict with the metric names as keys and the scores as values
    """

    textstat.set_lang("de")
    flesch_reading_ease = textstat.flesch_reading_ease(text)
    wiener_sachtextformel = textstat.wiener_sachtextformel(text, variant=4)

    res = {
        "flesch_reading_ease": flesch_reading_ease,
        "wiener_sachtextformel_4": wiener_sachtextformel,
    }
    return res


def calculate_rouge(predictions: list[str], references: list[str]) -> list[float]:
    """Calculate rouge2 between two lists of texts.

    :param predictions: list of predicted texts
    :param references: list of reference texts
    :return: list of rouge2 scores
    """
    rouge = evaluate.load("rouge")
    scores = rouge.compute(
        predictions=predictions,
        references=references,
        rouge_types=["rouge2"],
        use_aggregator=False,
    )
    return scores["rouge2"]


def recognize_language(texts: list[str]) -> list[str]:
    """Run automated language recognition on a list of texts, such as  `GERMAN` or `ENGLISH`.

    :param texts: list of texts
    :return: list of language names
    """
    languages = [Language.ENGLISH, Language.GERMAN]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()
    langs = detector.detect_languages_in_parallel_of(texts)
    lang_strings = [l.name if l else None for l in langs]
    return lang_strings


def split_text_naive(
    text: str, sep: str = None, separate_hyphens: bool = True
) -> list[str]:
    """Naive word splitting. Splits the text at the given separator and returns a list of words.
    Defaults to splitting at whitespace. By default, also replaces hyphens (which are commonly
    inserted into long words in Leichte Sprache) with the separator to account for the added
    readibility generated by this spelling.

    :param text: input string
    :param sep: separator. Default: None
    :param separate_hyphens: replace hyphens with the separator token to count hyphenated words as multiple words. Default:True
    :return: list of words
    """
    if separate_hyphens:
        replacement = " " if sep is None else sep
        text = text.replace("-", replacement)
    split = text.split(sep)
    return split


def calc_mean_word_length(split_text: list[str]) -> float:
    """Calculate the mean word length of a split (tokenized) text.
    Split the text using `score.split_text_naive` with your preferred
    parameters before using this function.

    :param split_text: text split into words
    :return: mean word length across the whole text
    """
    result = mean([len(w) for w in split_text])
    return result


def get_spacy_doc(text: str) -> spacy.Language:
    """Create a spacy document from a text.

    :param text: Input text
    :return: spacy doc
    """
    nlp = spacy.load(
        "de_core_news_sm", exclude=["ner", "tagger", "parser," "lemmatizer"]
    )
    doc = nlp(text)
    return doc


def calc_mean_sents_per_paragraph(paragraphs: list[list[str]]) -> float:
    """Take a list of paragraphs, each containing a list of the sentences
    in the paragraph. Calculate the mean number of sentences inside the paragraphs.

    :param paragraphs: list of sentences in each paragraph. Example: [["this is the first paragraph", "it has two sentences"], ["this is paragraph nr. 2"]]
    :return: mean paragraph length in sentences
    """
    if not paragraphs:
        return 0
    n_sents = [len(p) for p in paragraphs]
    mean_sents = mean(n_sents)
    return mean_sents


def split_paragraphs(sents: list) -> list[list[str]]:
    """Split a list of spacy sentences into a list of sentences per paragraph.

    :param sents: list of doc.sents
    :return: a list containing, for each paragraph, a list of the sentences in the paragraph
    """
    paragraphs = []

    current_par = []
    for sent in sents:
        current_par.append(sent)
        if "\n" in sent.text:
            paragraphs.append(current_par)
            current_par = []
    return paragraphs


def analyse_text_statistics(text: str) -> dict:
    """Run some quick statistics on the current text. Currently implemented are:

    - total number of sentences in the text
    - mean word length
    - mean paragraph length (in sentences)

    :param text: input text
    :return: dict with the metric names as keys and the results as values
    """

    # preprocess text
    naive_split = split_text_naive(text)
    doc = get_spacy_doc(text)
    sents = [sent for sent in doc.sents]
    paragraphs = split_paragraphs(sents)

    # calculate statistics
    mean_word_length = calc_mean_word_length(naive_split)
    total_sents_normalized = len(sents) / len(text)
    mean_sents_per_paragraph = calc_mean_sents_per_paragraph(paragraphs=paragraphs)

    res = {
        "total_sent_number_normalized": total_sents_normalized,
        "mean_word_length": mean_word_length,
        "mean_sents_per_paragraph": mean_sents_per_paragraph,
    }
    return res


def score_classification_set():
    """Calculate scores on the classification dataset."""
    # get the dataset
    dataset = load_dataset(os.getenv("HF_CLASSIFICATION_DATASET_NAME"), split="train")

    # run readibility, lexical diversity, avg sentence length, avg word length, avg text length?
    # dataset = dataset.map(lambda x: analyse_text_statistics(x["text"])) - DISABLED, NOT HELPFUL ENOUGH
    dataset = dataset.map(lambda x: calculate_readability_scores(x["text"]))
    dataset = dataset.train_test_split(test_size=0.1)

    dataset["train"].push_to_hub(
        os.getenv("HF_CLASSIFICATION_DATASET_NAME"),
        token=os.getenv("HF_TOKEN"),
        split="train",
    )
    dataset["test"].push_to_hub(
        os.getenv("HF_CLASSIFICATION_DATASET_NAME"),
        token=os.getenv("HF_TOKEN"),
        split="validation",
    )
    return


def run_rule_based_checks():
    # use short words, separate all long words with a hyphen
    # no abbreviations
    # use verbs rather than nouns
    # use active rather than passive
    # avoid genitive forms
    # avoid conjunctive forms
    # avoid negations
    # use arabic numbers only
    # avoid precise dates which are far in the past
    # avoid precise numbers (floating point, percentages)
    # prefer numbers (1) over number-words (one)
    # avoid special characters like " % ... ; & () $
    pass


def classifier_inference(model: PreTrainedModel, encoded_input, labels) -> tuple:
    """Run inference with the classifier.
    # todo: type hints

    :param model: classification model
    :param encoded_input: input encoded as pytorch tensor
    :param labels: class labels as tensor
    :return: tuple predicted_class_id, logits
    """
    try:
        output = model(**encoded_input, labels=labels)
        predicted_class_id = output.logits.argmax().item()
        logits = output.logits
    except (RuntimeError, IndexError) as e:
        logger.debug(f"Encountered error {e}. Setting prediciton and logits to None.")
        predicted_class_id, logits = None, None
    return predicted_class_id, logits


def run_classifier(
    model: PreTrainedModel, tokenizer: PreTrainedTokenizer, text: str
) -> dict:
    """Run the Leichte Sprache classifier. If the text input is too long for the model,
    it's split into chunks that can fit in the model and each chunk is classified
    separately. The predicted label and the logits are then calculated as a mean over all
    chunk results.

    :param model: classification model
    :param tokenizer: tokenizer of the classification model
    :param text: input text
    :return: tuple: predicted_class_id, logits
    """
    labels = torch.tensor([1]).unsqueeze(0)
    encoded_input = tokenizer(text, return_tensors="pt")
    n_tokens = len(encoded_input["input_ids"][0])

    if n_tokens > tokenizer.model_max_length:
        chunks = chunk_text(tokenizer, text, tokenizer.model_max_length * 0.9)
        pred_classes = []
        all_logits = []

        for chunk in chunks:
            encoded_chunk = tokenizer(chunk, return_tensors="pt")
            predicted_class_id, logits = classifier_inference(
                model, encoded_chunk, labels
            )
            if predicted_class_id is not None:
                pred_classes.append(predicted_class_id)
                all_logits.append(logits)
        predicted_class_id = round(mean(pred_classes))
        logits = torch.mean(torch.stack(all_logits), dim=0)
    else:
        predicted_class_id, logits = classifier_inference(model, encoded_input, labels)
    return {"class_id": predicted_class_id, "logits": logits}


def calc_share_newlines(text: str) -> dict:
    """Calculate the share of newslines in the text by counting them and
    normalizing them by the total number of characters. Mutliply by 100 to
    make the numbers easier to interpret at first glance.
    Values above 2 are typical for Leichte Sprache, values below 1 for standard
    German.

    :param text: text input
    :return: {"share_newlines": share_newlines}
    """
    if not text.strip():
        return {"share_newlines": 0}

    rx = r"\n+"
    subst = "\\n"
    normalized = re.sub(rx, subst, text)
    n_newlines = normalized.count("\n")
    share_newlines = n_newlines / len(text) * 100
    return {"share_newlines": share_newlines}


def calculate_metrics(
    model: AutoModelForSequenceClassification,
    tokenizer: AutoTokenizer,
    texts: list[str],
    original_texts: list[str],
    disable_tqdm: bool = False,
) -> pd.DataFrame:
    """Calculate metrics for the generated texts. Currently implemented:
    - Leichte Spreche classifier (predicted label, logits)
    - Readability metrics: Flesch reading ease, Wiener Sachtextformel
    - Rouge2: bigram overlap
    - Share newlines: number of newlines normalized by text length (>2 is typical for LS, <1 for SG)
    - Shared token length: 1.0 => texts have the same length, 0.1 LS => is much shorter, 1.5 => LS is much longer

    :param model: classification model
    :param tokenizer: tokenizer of the classification model
    :param texts: list of generated texts in Leichte Sprache
    :param original_texts: list of original texts in standard German, for calculating rouge and length diff
    :return: dataframe with the calculated metrics
    """
    scores = {
        "text_gen": texts,
        "text_orig": original_texts,
        "predicted_class": [],
        "logits_sg": [],
        "logits_ls": [],
        "flesch_reading_ease": [],
        "wiener_sachtextformel": [],
        "share_newlines": [],
    }

    for text in tqdm(texts, desc="Scoring generated texts", disable=disable_tqdm):
        classifier_res = run_classifier(model, tokenizer, text)
        readability = calculate_readability_scores(text)
        share_newlines = calc_share_newlines(text)
        scores["predicted_class"].append(classifier_res.get("class_id"))
        scores["logits_sg"].append(classifier_res.get("logits")[0][0].item())
        scores["logits_ls"].append(classifier_res.get("logits")[0][1].item())
        scores["flesch_reading_ease"].append(readability["flesch_reading_ease"])
        scores["wiener_sachtextformel"].append(readability["wiener_sachtextformel_4"])
        scores["share_newlines"].append(share_newlines.get("share_newlines"))

    gen_df = pd.DataFrame(scores)
    gen_df["diff_logits"] = abs(gen_df.logits_sg - gen_df.logits_ls)
    gen_df["shared_token_len"] = (
        gen_df.text_gen.str.split().str.len() / gen_df.text_orig.str.split().str.len()
    )
    gen_df["rouge2"] = calculate_rouge(texts, original_texts)
    return gen_df


if __name__ == "__main__":
    score_classification_set()
